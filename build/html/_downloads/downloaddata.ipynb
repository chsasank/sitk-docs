{
  "nbformat_minor": 0,
  "nbformat": 4,
  "metadata": {
    "language_info": {
      "mimetype": "text/x-python",
      "file_extension": ".py",
      "version": "3.4.5",
      "nbconvert_exporter": "python",
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "collapsed": false
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\ndownloaddata.py\n===============\n\nDownload this file and Data folder and add them to your python path to run other\nexamples.\n\nSince we do not want to store large binary data files in our Git repository,\nwe fetch_data_all from a network resource.\n\nThe data we download is described in a json file. The file format is a dictionary\nof dictionaries. The top level key is the file name. The returned dictionary\ncontains an md5 checksum and possibly a url and boolean flag indicating\nthe file is part of an archive. The md5 checksum is mandatory.\nWhen the optional url is given, we attempt to download from that url, otherwise\nwe attempt to download from the list of MIDAS servers returned by the \nget_midas_servers() function. Files that are contained in archives are\nidentified by the archive flag.\n\nExample json file contents:\n\n{\n\"SimpleITK.jpg\": {\n\"md5sum\": \"2685660c4f50c5929516127aed9e5b1a\"\n},\n\"POPI/meta/00.mhd\" : {\n\"md5sum\": \"3bfc3c92e18a8e6e8494482c44654fd3\",\n\"url\": \"http://tux.creatis.insa-lyon.fr/~srit/POPI/Images/MetaImage/10-MetaImage.tar\"\n},\n\"CIRS057A_MR_CT_DICOM/readme.txt\" : {\n \"md5sum\" : \"d92c97e6fe6520cb5b1a50b96eb9eb96\",\n \"archive\" : \"true\"\n}\n}\n\nNotes: \n1. The file we download can be inside an archive. In this case, the md5 \nchecksum is that of the archive.\n\n2. For the md5 verification to work we need to store archives on MIDAS and cannot\n   use its on-the-fly archive download mechanism (this mechanism allows users\n   to download \"directories/communities\" as a single zip archive). The issue is that\n   every time the archive is created its md5 changes. It is likely MIDAS is also\n   encoding the archive's modification/creation time as part of the md5.\n\n   Another issue is that when downloading from this type of url \n   (e.g. http://midas3.kitware.com/midas/download/folder/11610/ipythonNotebookData.zip)\n   the returned data does not have a \"Content-Length\" field in the header. The\n   current implementation  will throw an exception.  \n\n"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "execution_count": null,
      "source": [
        "import hashlib\nimport sys\nimport os\nimport json\n\nimport errno\nimport warnings\n\n# http://stackoverflow.com/questions/2028517/python-urllib2-progress-hook\n\ndef url_download_report(bytes_so_far, url_download_size, total_size):\n    percent = float(bytes_so_far) / total_size\n    percent = round(percent * 100, 2)\n    if bytes_so_far > url_download_size:\n        # Note that the carriage return is at the begining of the\n        # string and not the end. This accomodates usage in \n        # IPython usage notebooks. Otherwise the string is not\n        # displayed in the output.\n        sys.stdout.write(\"\\rDownloaded %d of %d bytes (%0.2f%%)\" %\n                         (bytes_so_far, total_size, percent))\n        sys.stdout.flush()\n    if bytes_so_far >= total_size:\n        sys.stdout.write(\"\\rDownloaded %d of %d bytes (%0.2f%%)\\n\" %\n                         (bytes_so_far, total_size, percent))\n        sys.stdout.flush()\n        \n \ndef url_download_read(url, outputfile, url_download_size=8192 * 2, report_hook=None):\n    # Use the urllib2 to download the data. The Requests package, highly\n    # recommended for this task, doesn't support the file scheme so we opted\n    # for urllib2 which does.  \n\n    try:\n        # Python 3\n        from urllib.request import urlopen, URLError, HTTPError\n    except ImportError:\n        from urllib2 import urlopen, URLError, HTTPError\n    from xml.dom import minidom\n    \n    # Open the url\n    try:\n        url_response = urlopen(url)\n    except HTTPError as e:\n        return \"HTTP Error: {0} {1}\\n\".format(e.code, url)\n    except URLError as e:\n        return \"URL Error: {0} {1}\\n\".format(e.reason, url)\n    # MIDAS is a service and therefor will not generate the expected URLError\n    # when given a nonexistent url. It does return an error message in xml.\n    # When the response is xml then we have an error, we read the whole message\n    # and return the 'msg' attribute associated with the 'err' tag.\n    # The URLError above is not superfluous as it will occur when the url \n    # refers to a non existent file ('file://non_existent_file_name') or url\n    # which is not a service ('http://non_existent_address').    \n    try:\n        # Python 3\n        content_type = url_response.info().get(\"Content-Type\")\n    except AttributeError:\n        content_type = url_response.info().getheader(\"Content-Type\")\n    # MIDAS error message in json format\n    if content_type == \"text/html; charset=UTF-8\":\n        doc = json.loads(url_response.read().decode(\"utf-8\"))\n        if doc['stat']=='fail':\n            return doc['message'] + url\n    # MIDAS error message in xml format\n    if content_type == \"text/xml\":\n        doc = minidom.parseString(url_response.read())\n        if doc.getElementsByTagName(\"err\")[0]:\n            return doc.getElementsByTagName(\"err\")[0].getAttribute(\"msg\") + ': ' + url\n    # We download all content types - the assumption is that the md5sum ensures\n    # that what we received is the expected data.\n    try:\n        # Python 3\n        content_length = url_response.info().get(\"Content-Length\")\n    except AttributeError:\n        content_length = url_response.info().getheader(\"Content-Length\")\n    total_size = content_length.strip()\n    total_size = int(total_size)\n    bytes_so_far = 0\n    with open(outputfile, \"wb\") as local_file:\n        while 1:\n            try:\n                url_download = url_response.read(url_download_size)\n                bytes_so_far += len(url_download)\n                if not url_download:\n                    break\n                local_file.write(url_download)\n            # handle errors\n            except HTTPError as e:\n                return \"HTTP Error: {0} {1}\\n\".format(e.code, url)\n            except URLError as e:\n                return \"URL Error: {0} {1}\\n\".format(e.reason, url)\n            if report_hook:\n                report_hook(bytes_so_far, url_download_size, total_size)\n    return \"Downloaded Successfully\"\n\n# http://stackoverflow.com/questions/600268/mkdir-p-functionality-in-python?rq=1\ndef mkdir_p(path):\n    try:\n        os.makedirs(path)\n    except OSError as exc:  # Python >2.5\n        if exc.errno == errno.EEXIST and os.path.isdir(path):\n            pass\n        else:\n            raise\n\n#http://stackoverflow.com/questions/2536307/decorators-in-the-python-standard-lib-deprecated-specifically\ndef deprecated(func):\n    \"\"\"This is a decorator which can be used to mark functions\n    as deprecated. It will result in a warning being emmitted\n    when the function is used.\"\"\"\n\n    def new_func(*args, **kwargs):\n        warnings.simplefilter('always', DeprecationWarning) #turn off filter \n        warnings.warn(\"Call to deprecated function {}.\".format(func.__name__), category=DeprecationWarning, stacklevel=2)\n        warnings.simplefilter('default', DeprecationWarning) #reset filter\n        return func(*args, **kwargs)\n\n    new_func.__name__ = func.__name__\n    new_func.__doc__ = func.__doc__\n    new_func.__dict__.update(func.__dict__)\n    return new_func\n\n\n\ndef get_midas_servers():\n    import os\n    midas_servers = list()\n    if 'ExternalData_OBJECT_STORES' in os.environ.keys():\n        local_object_stores = os.environ['ExternalData_OBJECT_STORES']\n        for local_object_store in local_object_stores.split(\";\"):\n          midas_servers.append( \"file://{0}/MD5/%(hash)\".format(local_object_store) )\n    midas_servers.extend( [\n        # Data published by MIDAS\n        \"http://midas3.kitware.com/midas/api/rest?method=midas.bitstream.download&checksum=%(hash)&algorithm=%(algo)\",\n        # Data published by developers using git-gerrit-push.\n        \"http://www.itk.org/files/ExternalData/%(algo)/%(hash)\",\n        # Mirror supported by the Slicer community.\n        \"http://slicer.kitware.com/midas3/api/rest?method=midas.bitstream.download&checksum=%(hash)&algorithm=%(algo)\",\n        # Insight journal data server\n        \"http://www.insight-journal.org/midas/api/rest?method=midas.bitstream.by.hash&hash=%(hash)\"\n        ])\n    return midas_servers\n\n\ndef output_hash_is_valid(known_md5sum, output_file):\n    md5 = hashlib.md5()\n    if not os.path.exists(output_file):\n        return False\n    with open(output_file, 'rb') as fp:\n        for url_download in iter(lambda: fp.read(128 * md5.block_size), b''):\n            md5.update(url_download)\n    retreived_md5sum = md5.hexdigest()\n    return retreived_md5sum == known_md5sum\n\n\ndef fetch_data_one(onefilename, output_directory, manifest_file, verify=True, force=False):\n    import tarfile, zipfile\n    \n    with open(manifest_file, 'r') as fp:\n        manifest = json.load(fp)\n    assert onefilename in manifest, \"ERROR: {0} does not exist in {1}\".format(onefilename, manifest_file)\n\n    sys.stdout.write(\"Fetching {0}\\n\".format(onefilename))\n    output_file = os.path.realpath(os.path.join(output_directory, onefilename))\n    data_dictionary = manifest[onefilename]\n    md5sum = data_dictionary['md5sum']    \n    # List of places where the file can be downloaded from\n    all_urls = []    \n    if \"url\" in data_dictionary:\n        all_urls.append(data_dictionary[\"url\"])    \n    else:\n        for url_base in get_midas_servers():\n            all_urls.append(url_base.replace(\"%(hash)\", md5sum).replace(\"%(algo)\", \"md5\"))\n        \n    new_download = False\n\n    for url in all_urls:\n        # Only download if force is true or the file does not exist.\n        if force or not os.path.exists(output_file):\n            mkdir_p(os.path.dirname(output_file))\n            # url_download_read(url, output_file, report_hook=url_download_report)\n            url_download_read(url, output_file, report_hook=None)\n            # Check if a file was downloaded and has the correct hash\n            if output_hash_is_valid(md5sum, output_file):\n                new_download = True\n                # Stop looking once found\n                break\n            # If the file exists this means the hash is invalid we have a problem.\n            elif os.path.exists(output_file):\n                    error_msg = \"File \" + output_file\n                    error_msg += \" has incorrect hash value, \" + md5sum + \" was expected.\"\n                    raise Exception(error_msg)\n\n    # Did not find the file anywhere.        \n    if not os.path.exists(output_file):\n        error_msg = \"File \" + \"\\'\"  + os.path.basename(output_file) +\"\\'\"\n        error_msg += \" could not be found in any of the following locations:\\n\" \n        error_msg += \", \".join(all_urls)\n        raise Exception(error_msg)\n    \n    if not new_download and verify:\n        # If the file was part of an archive then we don't verify it. These\n        # files are only verfied on download\n        if ( not \"archive\" in data_dictionary) and ( not output_hash_is_valid(md5sum, output_file) ):\n            # Attempt to download if md5sum is incorrect.\n            fetch_data_one(onefilename, output_directory, manifest_file, verify, \n                           force=True)\n    # If the file is in an archive, unpack it.                          \n    if tarfile.is_tarfile(output_file) or zipfile.is_zipfile(output_file):\n        tmp_output_file = output_file + \".tmp\"\n        os.rename(output_file, tmp_output_file)        \n        if tarfile.is_tarfile(tmp_output_file):\n            archive = tarfile.open(tmp_output_file)\n        if zipfile.is_zipfile(tmp_output_file):\n            archive = zipfile.ZipFile(tmp_output_file, 'r')\n        archive.extractall(os.path.dirname(tmp_output_file))\n        archive.close()\n        os.remove(tmp_output_file)\n\n    return output_file\n\n@deprecated\ndef fetch_midas_data_one(onefilename, output_directory, manifest_file, verify=True, force=False):\n    return fetch_data_one(onefilename, output_directory, manifest_file, verify, force)\n\n\ndef fetch_data_all(output_directory, manifest_file, verify=True):\n    with open(manifest_file, 'r') as fp:\n        manifest = json.load(fp)\n    for filename in manifest:\n        fetch_data_one(filename, output_directory, manifest_file, verify, \n                       force=False)\n\n@deprecated\ndef fetch_midas_data_all(output_directory, manifest_file, verify=True):\n    return fetch_data_all(output_directory, manifest_file, verify)\n\n\ndef fetch_data(cache_file_name, verify=False, cache_directory_name=\"Data\"):\n    \"\"\"\n    fetch_data is a simplified interface that requires\n    relative pathing with a manifest.json file located in the\n    same cache_directory_name name.\n\n    By default the cache_directory_name is \"Data\" relative to the current\n    python script.  An absolute path can also be given.\n    \"\"\"\n    if not os.path.isabs(cache_directory_name):\n        cache_root_directory_name = os.path.dirname(__file__)\n        cache_directory_name = os.path.join(cache_root_directory_name, cache_directory_name)\n    cache_manifest_file = os.path.join(cache_directory_name, 'manifest.json')\n    assert os.path.exists(cache_manifest_file), \"ERROR, {0} does not exist\".format(cache_manifest_file)\n    return fetch_data_one(cache_file_name, cache_directory_name, cache_manifest_file, verify=verify)\n\n@deprecated\ndef fetch_midas_data(cache_file_name, verify=False, cache_directory_name=\"Data\"):\n    return fetch_data(cache_file_name, verify, cache_directory_name)\n\n\nif __name__ == '__main__':\n    \n        \n    if len(sys.argv) < 3:\n        print('Usage: ' + sys.argv[0] + ' output_directory manifest.json')\n        sys.exit(1)\n    output_directory = sys.argv[1]\n    if not os.path.exists(output_directory):\n        os.makedirs(output_directory)\n    manifest = sys.argv[2]\n    fetch_data_all(output_directory, manifest)"
      ],
      "metadata": {
        "collapsed": false
      }
    }
  ]
}